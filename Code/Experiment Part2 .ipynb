{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10570674,"sourceType":"datasetVersion","datasetId":6541055},{"sourceId":10573152,"sourceType":"datasetVersion","datasetId":6542798},{"sourceId":10577715,"sourceType":"datasetVersion","datasetId":6545948},{"sourceId":10579197,"sourceType":"datasetVersion","datasetId":6546998},{"sourceId":10580692,"sourceType":"datasetVersion","datasetId":6548060},{"sourceId":10580693,"sourceType":"datasetVersion","datasetId":6548061},{"sourceId":10580810,"sourceType":"datasetVersion","datasetId":6548146},{"sourceId":10580818,"sourceType":"datasetVersion","datasetId":6548153},{"sourceId":10585424,"sourceType":"datasetVersion","datasetId":6550902},{"sourceId":10586617,"sourceType":"datasetVersion","datasetId":6551757},{"sourceId":10586782,"sourceType":"datasetVersion","datasetId":6551892},{"sourceId":10596282,"sourceType":"datasetVersion","datasetId":6558637},{"sourceId":10596343,"sourceType":"datasetVersion","datasetId":6558680},{"sourceId":10596417,"sourceType":"datasetVersion","datasetId":6558736},{"sourceId":10596489,"sourceType":"datasetVersion","datasetId":6558780}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture --no-stderr\n!pip install --upgrade --quiet  langchain langchain-community langchainhub beautifulsoup4 datasets","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-27T16:45:48.805222Z","iopub.execute_input":"2025-01-27T16:45:48.805595Z","iopub.status.idle":"2025-01-27T16:46:01.943275Z","shell.execute_reply.started":"2025-01-27T16:45:48.805557Z","shell.execute_reply":"2025-01-27T16:46:01.941489Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('csv', data_files = \"/kaggle/input/dataset/baseline_data_complete.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"JUDGE_PROMPT_Faithfulness = \"\"\"\nYou will be given a question, an answer, a context, and a ground truth pair. Your task is to extract all atomic facts from the answer and the corresponding part in the context that related the questions in the form of triples: (subject, predicate, object). Each fact should be represented as a triple where:\n\n- `subject`: the entity performing or being described.\n- `predicate`: the action, relation, or property of the subject.\n- `object`: the target, recipient, or detail associated with the predicate.\n\nYour task is to ensure that:\n\n1. Each triple contains exactly one subject, one predicate, and one object.\n2. Please only use the word in the context or answer\n3. If a sentence contains multiple facts, list them all as separate triples.\n4. You must use all provided context.\n\nNow here are the details:\nQuestion：{question}\nAnswer: {answer}\nContext: {context}\n\nThe structure of your feedback should be as follows:\n\nFeedback::\n\nContext Facts:\n1. (subject: subject, predicate: predicate, object: object)\n2. (subject: subject, predicate: predicate, object: object)\n\nAnswer Facts:\n1. (subject: subject, predicate: predicate, object: object)\n2. (subject: subject, predicate: predicate, object: object)\n\nMake sure that each fact from the answer is supported by an appropriate fact from the context in the same order.\n\"\"\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-25T10:02:55.691550Z","iopub.execute_input":"2025-01-25T10:02:55.692001Z","iopub.status.idle":"2025-01-25T10:02:55.697908Z","shell.execute_reply.started":"2025-01-25T10:02:55.691964Z","shell.execute_reply":"2025-01-25T10:02:55.696494Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"JUDGE_PROMPT_Response_Relevancy = \"\"\"\nYou will be given a context.\nYour task is to generate the related questions based on the contexts. The idea is to see what kind of questions will need this context.\n\nHere are the steps for evaluation:\n\n### 1. **Generate questions**:\n- Based on the context given, generated 3 different corresponding questions. The question should be able to reflect the content of the context.\n- Please do not contain any specific information pointing to context, like \"first context\" or \"second context\"\n### 2. **Provide Feedback**:\n- **Generated Questions**: 3 AI-generated questions for the context\n\n**Instructions for the evaluation**:\n- Carefully read the question, answer, context, and ground truth.\n- Generate 3 questions for the based on the contexts.\n- Provide the **Generated Questions**\n\nNow here are the details:\n\nContext: {context}\n\nProvide your feedback with the following format:\n\nFeedback::: \nGenerated Questions: \n1. question1\n2. quetions2\n3. question3\n\nPLEASE Do Not GIVE ANY OTHER INFORMATION.\nThank you for your evaluation.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:34:54.138382Z","iopub.execute_input":"2025-01-24T21:34:54.138740Z","iopub.status.idle":"2025-01-24T21:34:54.143033Z","shell.execute_reply.started":"2025-01-24T21:34:54.138709Z","shell.execute_reply":"2025-01-24T21:34:54.141934Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"JUDGE_PROMPT_Factual Correctness = \"\"\"\nYou will be given an answer and ground truth pair, both in (subject, predicate, object) tuple.\nYour task is to provide a 'Factual Correctness' score, evaluating how factually consistent the generated answer is with the ground truths. The score ranges from 0 to 1, where 1 indicates perfect consistency.\n\nHere are the steps for evaluation:\n\n### 2. **Evaluate Factual Correctness**:\n- After extracting the atomic facts, evaluate how consistent each fact is with the **retrieved context**. Check if the facts are supported by information from the context.\n- **True Positive, False Negative and False Positive Formular**:  \n  True Positive (TP) = Number of statements in generated answers that are present in ground truths\n  False Negative (FN) = Number of statements in ground truth that are not present in generated answers\n \n- **Factual Correctness Formula**\n  |Factual Correctness| = TP / (TP + FN)\n### 3. **Provide Feedback**:\n- **Factual Correctness**: A score between 0 and 1 that represents how factually consistent the response is with the reference. \n\n**Instructions for the evaluation**:\n- Carefully read the answer and ground truth.\n- Check each answer fact against the ground truths facts to determine if it’s supported.\n- Check ground truths facts against the each answer fact to determine if it’s supported.\n- Calculate the TP, FP, FN value \n- calculate the  Factual Correctness with the Formula\n- Provide a **Factual Correctness** score.\n\nNow here are the details:\n\nAnswer: {answer}\nGround Truth: {ground_truth}\n\nProvide your feedback with the following format:\n\nFeedback:::\nFactual Correctness: [Value between 0 and 1]  \n \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:14:28.099027Z","iopub.execute_input":"2025-01-27T21:14:28.099426Z","iopub.status.idle":"2025-01-27T21:14:28.104750Z","shell.execute_reply.started":"2025-01-27T21:14:28.099397Z","shell.execute_reply":"2025-01-27T21:14:28.103404Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"def extract_judge_score(answer: str, split_str: str) -> int:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n        return float(digit_groups[0])\n    except Exception as e:\n        print(e)\n        return None\n\ndef extract_judge_score(answer: str, split_str: str) -> int:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n        return float(digit_groups[0])\n    except Exception as e:\n        print(e)\n        return None\n\ndef extract_judge_facts(answer: str, split_str: str) -> list:\n    try:\n            # If split_str is present in the answer, split the answer accordingly\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        \n        # Use a regex pattern to capture each atomic factual statement (numbered items)\n        facts = re.findall(r'(\\d+)\\.\\s*\\((.*?)\\)', rating)\n        \n        # Convert the extracted data into a structured list of tuples (numbered fact, atomic fact)\n        extracted_facts = [(fact[0], fact[1]) for fact in facts]\n        \n        return extracted_facts\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\nimport re\n\ndef extract_judge_questions(answer: str, split_str: str = \"Feedback::: Generated Questions:\") -> list:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        questions = re.findall(r'(\\d+)\\.\\s*(.+)', rating)\n        extracted_questions = [q[1].strip() for q in questions]\n        return extracted_questions\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\ndef generate_improved_judge_response(row):\n    prompt = JUDGE_PROMPT.format(answer=row['Answer Facts String'], ground_truth=row['Related Grounds String'])\n    #print(prompt)\n    client = OpenAI(\n        api_key=\"sk-3bJrZlYFjViYQEDsC2D6F3F9B3Ae46208eDfDdE7E24dE512\",\n        base_url=\"https://api.openai.com/v1\"\n    )\n    client.base_url = \"https://ai-yyds.com/v1\"\n\n    response = client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are proficient in extract atomic facts from sentences and compare the meaning of different sentences.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"gpt-4o-mini\"\n    )\n    generated_text  = response.choices[0].message.content\n    return generated_text\n\n\ndef extract_facts(answer: str) -> dict:\n    try:\n        # 定义结果字典\n        results = {\"Context Facts\": [], \"Answer Facts\": []}\n        \n        # 定义分割标记\n        context_maker = \"Context Facts:\"\n        answer_marker = \"Answer Facts:\"\n        \n        context_part = re.search(f\"{context_maker}(.*?){answer_marker}\", answer, re.DOTALL)\n        answer_part = re.search(f\"{answer_marker}(.*)\", answer, re.DOTALL)\n        \n        def parse_facts(text):\n            facts = re.findall(r'\\(\\s*subject:\\s*(.*?),\\s*predicate:\\s*(.*?),\\s*object:\\s*(.*?)\\s*\\)', text)\n            return [{\"subject\": fact[0].strip(), \"predicate\": fact[1].strip(), \"object\": fact[2].strip()} for fact in facts]\n        \n        if context_part:\n            context_text = context_part.group(1).strip()\n            results[\"Context Facts\"] = parse_facts(context_text)\n        \n        if answer_part:\n            answer_text = answer_part.group(1).strip()\n            results[\"Answer Facts\"] = parse_facts(answer_text)\n        \n        return results\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {}\n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:14:31.476404Z","iopub.execute_input":"2025-01-27T21:14:31.476754Z","iopub.status.idle":"2025-01-27T21:14:31.492999Z","shell.execute_reply.started":"2025-01-27T21:14:31.476727Z","shell.execute_reply":"2025-01-27T21:14:31.491553Z"}},"outputs":[],"execution_count":127},{"cell_type":"code","source":"def build_fact_sentences(answer_facts):\n    fact_sentences = [\n        f\"{index + 1}. (Subject:{fact['subject']}, Predicate:{fact['predicate']}, Object:{fact['object']})\"\n        for index, fact in enumerate(answer_facts)\n    ]\n    return \"\\n\".join(fact_sentences) \n\ndef build_related_context_string(related_contexts):\n    return \" \".join(related_contexts)\n\ndef process_data_list(data_list):\n    results = []\n    for data in data_list:\n        answer_facts_string = build_fact_sentences(data['Answer Facts'])\n        related_context_string = build_fact_sentences(data['Ground Truth Facts'])\n        result_dict = {\n            \"Answer Facts String\": answer_facts_string,\n            \"Related Grounds String\": related_context_string\n        }\n        results.append(result_dict)\n    return results\n\nresult_list = process_data_list(loaded_facts_results)\nprint(result_list[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:18:30.640357Z","iopub.execute_input":"2025-01-27T21:18:30.640727Z","iopub.status.idle":"2025-01-27T21:18:30.667192Z","shell.execute_reply.started":"2025-01-27T21:18:30.640691Z","shell.execute_reply":"2025-01-27T21:18:30.665987Z"}},"outputs":[{"name":"stdout","text":"{'Answer Facts String': '1. (Subject:The music director of the Quebec Symphony Orchestra, Predicate:is, Object:Fabien Gabel)', 'Related Grounds String': '1. (Subject:The music director of the Quebec Symphony Orchestra, Predicate:is, Object:Fabien Gabel)'}\n","output_type":"stream"}],"execution_count":136},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/dataset/baseline_data_complete.csv\")\ndef clean_context(context):\n    cleaned_context = context.lstrip(\"'[\\\"\")\n    cleaned_context = cleaned_context.rstrip(\"]\\\"'\")\n\n    return cleaned_context\n\n    return cleaned_context\n\ndef update_ground_truths(example):  \n    try:  \n        # Iterate over the contexts in the \"ground_truths\" field\n            # Strip unwanted characters from both ends of each context string\n        cleaned_context = clean_context(example['contexts'])\n        # Update the context in the ground_truths list with the cleaned context\n        example[\"contexts\"] = cleaned_context\n    except Exception as e:  \n        print(f\"Error: {e}\")  \n    \n    return example  \n\ndataset = dataset.map(update_ground_truths)\nprint(result_list[0])\n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:18:17.398505Z","iopub.execute_input":"2025-01-27T21:18:17.398943Z","iopub.status.idle":"2025-01-27T21:18:17.702594Z","shell.execute_reply.started":"2025-01-27T21:18:17.398908Z","shell.execute_reply":"2025-01-27T21:18:17.701581Z"}},"outputs":[{"name":"stdout","text":"{'Answer Facts String': '1. (Subject:The music director of the Quebec Symphony Orchestra, Predicate:is, Object:Fabien Gabel)', 'Related Grounds String': '1. (Subject:The music director of the Quebec Symphony Orchestra, Predicate:is, Object:Fabien Gabel)'}\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"from tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport openai\nimport csv\nimport time\nfrom datasets import load_dataset\nfrom threading import Semaphore\nfrom openai import OpenAI\n\nMAX_RATE = 5  #\nsemaphore = Semaphore(MAX_RATE)\ndef process_data(i, data):\n    try:\n        with semaphore:\n            start_time = time.time()\n\n            report = generate_improved_judge_response(data)\n            facts = extract_judge_score(report, \"Factual Correctness:\") \n\n            elapsed_time = time.time() - start_time\n            if elapsed_time < 1 / MAX_RATE:\n                time.sleep(1 / MAX_RATE - elapsed_time)\n\n        return i, facts\n\n    except Exception as e:\n        print(f\"Error in process_data for index {i}: {e}\")\n        return i, []  # 返回默认空 facts\n\nfacts_results = [None] * len(result_list)\n\nwith ThreadPoolExecutor(max_workers=MAX_RATE) as executor:\n    futures = [executor.submit(process_data, data, result_list[data]) for i, data in enumerate(indexes)]\n\n    for future in tqdm(futures, total=len(futures)):\n        try:\n            i, facts = future.result()\n            facts_results[i] = facts\n        except Exception as e:\n            print(f\"Error processing future: {e}\")\n            facts_results[i] = -1\n        \nprint(\"Processing complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T21:18:51.734967Z","iopub.execute_input":"2025-01-27T21:18:51.735324Z","iopub.status.idle":"2025-01-27T21:35:55.416479Z","shell.execute_reply.started":"2025-01-27T21:18:51.735297Z","shell.execute_reply":"2025-01-27T21:35:55.414384Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 207/207 [17:03<00:00,  4.95s/it]","output_type":"stream"},{"name":"stdout","text":"Processing complete.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":137},{"cell_type":"code","source":"import json\nwith open(\"/kaggle/working/new_questions.json\", \"r\", encoding=\"utf-8\") as file:\n    loaded_facts_results = json.load(file)\n\nprint(loaded_facts_results[8])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T22:42:45.743624Z","iopub.execute_input":"2025-01-24T22:42:45.744072Z","iopub.status.idle":"2025-01-24T22:42:45.760510Z","shell.execute_reply.started":"2025-01-24T22:42:45.744032Z","shell.execute_reply":"2025-01-24T22:42:45.759258Z"}},"outputs":[{"name":"stdout","text":"['What makes the TIFF Bell Lightbox a unique venue for screening classic films?', 'How does the audience\\'s diversity enhance the experience of watching \"Dial M for Murder\"?', 'What factors contribute to the acclaim of \"Dial M for Murder\" as a classic film?']\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"for i, question in enumerate(facts_results):\n    if question != None:\n       loaded_facts_results[i] = question\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T21:49:34.789177Z","iopub.execute_input":"2025-01-24T21:49:34.789579Z","iopub.status.idle":"2025-01-24T21:49:34.795012Z","shell.execute_reply.started":"2025-01-24T21:49:34.789552Z","shell.execute_reply":"2025-01-24T21:49:34.793848Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from tqdm import tqdm\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings \nimport openai\nfrom openai import OpenAI\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\ndataset = load_dataset('csv', data_files=\"/kaggle/input/dataset/baseline_data_complete.csv\")\nclient = OpenAI(\n    api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n    base_url=\"https://api.openai.com/v1\"\n)\nclient.base_url=\"https://ai-yyds.com/v1\"\n    \ndef get_text_embedding(text):\n    embedding = client.embeddings.create(input=text, model=\"text-embedding-3-large\"\n\n).data[0].embedding\n    return embedding\n\ndef calculate_cosine_similarity(vector1, vector2):\n    return cosine_similarity([vector1], [vector2])[0][0]\n    \nresults = []\nfor i in tqdm(range(len(indexes))):\n    similarities = []\n    emb1 = get_text_embedding(dataset['train']['question'][indexes[i][\"index\"]])\n    for question in loaded_facts_results[indexes[i][\"index\"]]:\n        emb2 = get_text_embedding(question)\n        similarity = calculate_cosine_similarity(emb1,emb2)\n        similarities.append(similarity)\n    results.append({\"index\": indexes[i][\"index\"], \"similarities\": similarities})\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T22:15:56.819302Z","iopub.execute_input":"2025-01-24T22:15:56.819658Z","iopub.status.idle":"2025-01-24T22:27:25.823017Z","shell.execute_reply.started":"2025-01-24T22:15:56.819630Z","shell.execute_reply":"2025-01-24T22:27:25.821868Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 424/424 [11:28<00:00,  1.62s/it]\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"from tqdm import tqdm\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings \nimport openai\nfrom openai import OpenAI\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\ndataset = load_dataset('csv', data_files=\"/kaggle/input/dataset/baseline_data_complete.csv\")\nclient = OpenAI(\n    api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n    base_url=\"https://api.openai.com/v1\"\n)\nclient.base_url=\"https://ai-yyds.com/v1\"\ndef get_text_embedding(text):\n    embedding = client.embeddings.create(input=text, model=\"text-embedding-3-large\"\n\n).data[0].embedding\n    return embedding\n\ndef calculate_cosine_similarity(vector1, vector2):\n    return cosine_similarity([vector1], [vector2])[0][0]\nsimilarities = []\nemb1 = get_text_embedding(dataset['train']['question'][1790])\nfor question in loaded_facts_results[1790]:\n    emb2 = get_text_embedding(question)\n    similarity = calculate_cosine_similarity(emb1,emb2)\n    similarities.append(similarity)\nprint(similarities)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T22:42:49.909662Z","iopub.execute_input":"2025-01-24T22:42:49.909992Z","iopub.status.idle":"2025-01-24T22:42:50.872808Z","shell.execute_reply.started":"2025-01-24T22:42:49.909964Z","shell.execute_reply":"2025-01-24T22:42:50.871716Z"}},"outputs":[{"name":"stdout","text":"[]\n","output_type":"stream"}],"execution_count":75}]}