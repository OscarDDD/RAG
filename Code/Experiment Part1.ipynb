{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10443790,"sourceType":"datasetVersion","datasetId":6464391},{"sourceId":10489628,"sourceType":"datasetVersion","datasetId":6494702},{"sourceId":10497865,"sourceType":"datasetVersion","datasetId":6499843},{"sourceId":10499563,"sourceType":"datasetVersion","datasetId":6500823},{"sourceId":10500083,"sourceType":"datasetVersion","datasetId":6501103},{"sourceId":10500087,"sourceType":"datasetVersion","datasetId":6501106},{"sourceId":10564769,"sourceType":"datasetVersion","datasetId":6537484},{"sourceId":10564934,"sourceType":"datasetVersion","datasetId":6537597},{"sourceId":10576692,"sourceType":"datasetVersion","datasetId":6545252},{"sourceId":10585648,"sourceType":"datasetVersion","datasetId":6551074},{"sourceId":10586142,"sourceType":"datasetVersion","datasetId":6551442},{"sourceId":10595227,"sourceType":"datasetVersion","datasetId":6557863},{"sourceId":10595292,"sourceType":"datasetVersion","datasetId":6557910}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture --no-stderr\n!pip install --upgrade --quiet  langchain langchain-community langchainhub beautifulsoup4 datasets\n!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille datasets\n!pip install langchain-openai\n!pip install ragas\n!pip install -U bitsandbytes\n!pip install transformers accelerate bitsandbytes flash-attn\n!pip install sentence-transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bs4\nfrom langchain import hub\nimport torch\nfrom typing import List, Optional\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.chains import create_retrieval_chain\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI\nfrom openai import OpenAI\nfrom datasets import load_dataset\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nfrom transformers import AutoTokenizer\nimport tqdm\n\nEMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n\nds = load_dataset(\"neural-bridge/rag-dataset-12000\")\ntrain_ds = ds[\"train\"]\ntest_ds = ds[\"test\"]\n\nRAW_KNOWLEDGE_BASE = [\n    LangchainDocument(\n        page_content=doc[\"context\"],\n    )\n    for i, doc in enumerate(test_ds)\n]\n\ndef split_documents(\n    chunk_size: int,\n    knowledge_base: List[LangchainDocument],\n    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n) -> List[LangchainDocument]:\n    \"\"\"\n    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique\n    \n\n#docs_processed = split_documents(\n#    512,  # We choose a chunk size adapted to our model\n#    RAW_KNOWLEDGE_BASE,\n#    tokenizer_name=EMBEDDING_MODEL_NAME,\n#)\n\ndocs_processed = []\nfor doc in RAW_KNOWLEDGE_BASE:\n    docs_processed += [doc]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd \nimport matplotlib.pyplot as plt\ntokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\nlengths = [len(tokenizer.encode(docs_processed[i].page_content)) for i in range(len(docs_processed))]\nfig = pd.Series(lengths).hist()\nplt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from langchain.schema import BaseRetriever\nfrom pydantic import BaseModel\n\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)\n\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n        \nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever()\n\n# 2. Incorporate the retriever into a question-answering chain.\nsystem_prompt = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer \"\n    \"the question. If you don't know the answer, say that you \"\n    \"don't know. Use three sentences maximum and keep the \"\n    \"answer concise.\"\n    \"\\n\\n\"\n    \"{context}\"\n)\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", system_prompt),\n        (\"human\", \"{input}\"),\n    ]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### from langchain.llms.base import LLM\nfrom typing import List, Optional\nimport requests\nfrom datasets import Dataset\nfrom langchain.chat_models import ChatOpenAI\n\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n                openai_api_base=\"https://ai-yyds.com/v1\")\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\ndataset_base = load_dataset(\"csv\", data_files = \"/kaggle/input/complete-base-data/baseline_data_complete.csv\")\nanswers = []\nquestions = []\ncontexts = []\nground_truths = []\ntry:\n    for index in indexes:\n        result = rag_chain.invoke({\"input\": dataset_base['train'][\"question\"]})['answer']\n        context = [docs.page_content for docs in retriever.get_relevant_documents(dataset_base['train'][\"question\"])]\n        answers.append(result)\n        contexts.append(context)\n        questions.append(dataset_base['train'][index][\"question\"])\n        ground_truths.append(dataset_base['train'][index][\"ground_truths\"])\n    print(len(context))\n    print(\"Generated Answer:\", context)\nexcept Exception as e:\n    print(f\"Error during chain invocation: {e}\")\n\n#data = {\n#    \"question\":questions,\n #   \"answer\": answers,\n ##   \"contexts\": contexts,\n  #  \"ground_truths\": ground_truths\n#}\n# Convert dict to dataset\n#dataset = Dataset.from_dict(data) \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nanswers = []\ncontexts = []\nground_truths = []\nbatch_size = 108\nquestions = test_ds[\"question\"]\nqueries = []\n# Split the questions into batches of 108\nbatches = [\n    [(i, questions[i]) for i in range(start, min(start + batch_size, len(questions)))]\n    for start in range(0, len(questions), batch_size)\n]\n\ndef process_query(query, id):\n   try:\n        answer = rag_chain.invoke({\"input\": query})\n        \n        context = [docs.page_content for docs in retriever.get_relevant_documents(query)]\n        \n        return answer[\"answer\"], context, id\n   except Exception as e:\n        print(f\"Error processing query ID {id}: {e}\")\n        return None, None, id\n\ndef process_batch(batch):\n    batch_answers = []\n    batch_contexts = []\n    \n    # Use a ThreadPoolExecutor but for smaller batches\n    with ThreadPoolExecutor(max_workers=8) as executor:  # Limit number of threads to prevent overload\n        futures = {executor.submit(process_query, query, id): query for (id, query) in batch}\n        \n        for future in tqdm(as_completed(futures), total=len(futures)):\n            try:\n                answer, context, id = future.result()\n                if answer is not None and context is not None:  # 只处理成功的结果\n                    batch_answers.append(answer)\n                    batch_contexts.append(context)\n                    ground_truths.append(test_ds[\"answer\"][id])\n                    queries.append(test_ds[\"question\"][id])\n            except Exception as e:\n                print(f\"Error in future processing: {e}\")\n    return batch_answers, batch_contexts\n    \nfor batch in tqdm(batches):\n    batch_answers, batch_contexts = process_batch(batch)\n    answers.extend(batch_answers)\n    contexts.extend(batch_contexts)\n    \n# To dict\ndata = {\n    \"question\":queries,\n    \"answer\": answers,\n    \"contexts\": contexts,\n    \"reference\": ground_truths\n}\n\n# Convert dict to dataset\ndataset = Dataset.from_dict(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nJUDGE_PROMPT = \"\"\"\nYou will be given the answer and ground truth couple.\nYour task is to extract all atomic facts from a answers and ground truths in the form of triples: (subject, predicate, object). Each fact must be expressed as a triple where:\n- `subject` is the entity performing or being described.\n- `predicate` is the action, relation, or property of the subject.\n- `object` is the target, recipient, or detail associated with the predicate.\n\nEnsure that:\n1. Each triple contains exactly one subject, one predicate, and one object.\n2. If a sentence contains multiple facts, list them all as separate triples.\n3. Be precise and concise in extracting triples from complex sentences.\n\nYOU MUST GIVE the Ground Truth Facts: and Answer Facts: in your feedback\n\nNow here are the question, answer, contexts and ground truths.\n\nAnswer: {answer}\nGroundTruth: {groundTruth}\n\nProvide your feedback as follows:\n\nFeedback:::\nGround Truth Facts:\n1. (subject: subject, predicate:predicate, object:object)\n2. (subject: subject, predicate:predicate, object:object)\n...\n\nAnswer Facts:\n1. (subject: subject, predicate:predicate, object:object)\n2. (subject: subject, predicate:predicate, object:object)\n...\nExample: (subject: I, predicate: come, object: home)\n\"\"\"\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"JUDGE_PROMPT = \"\"\"\nYou will be given a question, answer, context, and ground truth pair.\nYour task is to generate the related questions based on the contexts. The idea is to see what kind of questions will need this context.\n\nHere are the steps for evaluation:\n\n### 1. **Generate questions**:\n- Based on the context given, generated 3 different corresponding questions. The question should be able to reflect the content of the context.\n\n### 2. **Provide Feedback**:\n- **Generated Questions**: 3 AI-generated questions for the context\n\n**Instructions for the evaluation**:\n- Carefully read the question, answer, context, and ground truth.\n- Generate 3 questions for the based on the contexts.\n- Provide the **Generated Questions**\n\nNow here are the details:\n\nQuestion: {question}\nAnswer: {answer}\nContext: {context}\nGroundTruth: {groundTruth}\n\nProvide your feedback with the following format:\n\nFeedback::: \nGenerated Questions: \n1. question1\n2. quetions2\n3. question3\n\nPLEASE Do Not GIVE ANY OTHER INFORMATION.\nThank you for your evaluation.\n\n \"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"JUDGE_PROMPT = \"\"\"\nYou will be given a question, answer, context pair. Your task is to extract all **atomic facts** from the answer in the form of triples: (subject, predicate, object). Each fact should be represented as a triple where:\n\n- `subject`: the entity performing or being described.\n- `predicate`: the action, relation, or property of the subject.\n- `object`: the target, recipient, or detail associated with the predicate.\n\nYou also need to extract the **related contexts**: the corresponding part in the context that supports the atomic facts you generated.\nYour task is to ensure that:\n\n1. Each triple contains exactly one subject, one predicate, and one object.\n2. Please use the original parts in contexts for **related contexts**\n3. If a sentence contains multiple facts, list them all as separate triples.\n4. If one context supports multiple answers, please repeat the context for multiple times with the same order of the answer facts.\n5. You must keep the order of the **Related Context** and **Answer Facts** the same. It means, if the first related context must support the first atomic facts.\n6. If no supported context can be found leave an empty string there.\n\nNow here are the details:\nQuestion：{question}\nAnswer: {answer}\nContext: {context}\n\nThe structure of your feedback should be as follows:\n\nFeedback::\n\nRelated Context:\n1. context1\n2. context2\n\nAnswer Facts:\n1. (subject: subject, predicate: predicate, object: object)\n2. (subject: subject, predicate: predicate, object: object)\n\nMake sure that each fact from the answer is supported by an appropriate fact from the context in the same order.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_judge_score(answer: str, split_str: str) -> int:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n        return float(digit_groups[0])\n    except Exception as e:\n        print(e)\n        return None\n\ndef extract_judge_score(answer: str, split_str: str) -> int:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        digit_groups = [el.strip() for el in re.findall(r\"\\d+(?:\\.\\d+)?\", rating)]\n        return float(digit_groups[0])\n    except Exception as e:\n        print(e)\n        return None\n\ndef extract_judge_facts(answer: str, split_str: str) -> list:\n    try:\n        # If split_str is present in the answer, split the answer accordingly\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        \n        # Use a regex pattern to capture each atomic factual statement (numbered items)\n        facts = re.findall(r'(\\d+)\\.\\s*\\((.*?)\\)', rating)\n        \n        # Convert the extracted data into a structured list of tuples (numbered fact, atomic fact)\n        extracted_facts = [(fact[0], fact[1]) for fact in facts]\n        \n        return extracted_facts\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\nimport re\n\ndef extract_judge_questions(answer: str, split_str: str = \"Feedback::: Generated Questions:\") -> list:\n    try:\n        if split_str in answer:\n            rating = answer.split(split_str)[1]\n        else:\n            rating = answer\n        \n        questions = re.findall(r'(\\d+)\\.\\s*(.+)', rating)\n        \n        extracted_questions = [q[1].strip() for q in questions]\n        \n        return extracted_questions\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\ndef generate_improved_judge_response(row):\n    prompt = JUDGE_PROMPT.format(question=row['question'], answer=row['answer'], context=row['contexts'], groundTruth = row['ground_truths'])\n    #print(prompt)\n    client = OpenAI(\n        api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n        base_url=\"https://api.openai.com/v1\"\n    )\n    client.base_url = \"https://ai-yyds.com/v1\"\n\n    response = client.chat.completions.create(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are proficient in extract atomic facts from sentences and compare the meaning of different sentences.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        model=\"gpt-4o-mini\"\n    )\n    generated_text  = response.choices[0].message.content\n    return generated_text\n\n\ndef extract_facts(answer: str) -> dict:\n    try:\n        results = {\"Ground Truth Facts\": [], \"Answer Facts\": []}\n        \n        context_marker = \"Ground Truth Facts:\"\n        answer_marker = \"Answer Facts:\"\n        \n        context_part = re.search(f\"{context_marker}(.*?){answer_marker}\", answer, re.DOTALL)\n        answer_part = re.search(f\"{answer_marker}(.*)\", answer, re.DOTALL)\n        \n        def parse_facts(text):\n            facts = re.findall(r'\\(\\s*subject:\\s*(.*?),\\s*predicate:\\s*(.*?),\\s*object:\\s*(.*?)\\s*\\)', text)\n            return [{\"subject\": fact[0].strip(), \"predicate\": fact[1].strip(), \"object\": fact[2].strip()} for fact in facts]\n        \n        if context_part:\n            context_text = context_part.group(1).strip()\n            # Split by lines and remove numbering (e.g., 1., 2., ...)\n            #cleaned_lines = [re.sub(r'^\\d+\\.\\s*', '', line).strip() for line in lines if line.strip()]  # Clean and skip empty lines\n            results[\"Ground Truth Facts\"] = parse_facts(context_text)  # Store each line as a separate list item\n        else:\n            results[\"Ground Truth Facts\"] = []\n        \n        if answer_part:\n            answer_text = answer_part.group(1).strip()\n            results[\"Answer Facts\"] = parse_facts(answer_text)\n        \n        return results\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {}\n\ndef extract_new_facts(answer, context_marker=\"Related Context:\", answer_marker=\"Answer Facts\"):\n    results = {}\n\n    try:\n        context_part = re.search(f\"{context_marker}(.*?){answer_marker}\", answer, re.DOTALL)\n        answer_part = re.search(f\"{answer_marker}(.*)\", answer, re.DOTALL)\n        \n        def parse_facts(text):\n            facts = re.findall(r'\\(\\s*subject:\\s*(.*?),\\s*predicate:\\s*(.*?),\\s*object:\\s*(.*?)\\s*\\)', text)\n            return [{\"subject\": fact[0].strip(), \"predicate\": fact[1].strip(), \"object\": fact[2].strip()} for fact in facts]\n        \n        if context_part:\n            context_text = context_part.group(1).strip()\n            # Split by lines and remove numbering (e.g., 1., 2., ...)\n            lines = context_text.splitlines()\n            cleaned_lines = [re.sub(r'^\\d+\\.\\s*', '', line).strip() for line in lines if line.strip()]  # Clean and skip empty lines\n            results[\"Related Context\"] = cleaned_lines  # Store each line as a separate list item\n        else:\n            results[\"Related Context\"] = []\n\n        if answer_part:\n            answer_text = answer_part.group(1).strip()\n            results[\"Answer Facts\"] = parse_facts(answer_text)\n        else:\n            results[\"Answer Facts\"] = [] \n        \n        return results\n    \n    except Exception as e:\n        print(f\"Error: {e}\")\n        return {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings \nimport openai\nfrom openai import OpenAI\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\n#dataset = load_dataset('csv', data_files=\"/kaggle/input/dataset/baseline_data_complete.csv\")\nclient = OpenAI(\n    api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n    base_url=\"https://api.openai.com/v1\"\n)\nclient.base_url=\"https://ai-yyds.com/v1\"\n    \ndef get_text_embedding(text):\n    embedding = client.embeddings.create(input=text, model=\"text-embedding-3-large\"\n\n).data[0].embedding\n    return embedding\n\ndef process_fact(fact, contexts):\n    emb1 = get_text_embedding(build_from_facts(fact))\n    similarities = []\n    for context in contexts:\n        emb2 = get_text_embedding(build_from_facts(context)) \n        similarity = calculate_cosine_similarity(emb1, emb2)\n        similarities.append(similarity)\n    return similarities\n\ndef process_data_multithreaded(loaded_facts_results):\n    results = [None] * len(loaded_facts_results) \n    with ThreadPoolExecutor() as executor:\n        futures = []\n        for i, data in enumerate(loaded_facts_results):\n            contexts = data['Ground Truth Facts']\n            facts = data['Answer Facts']\n\n            future = executor.submit(process_facts_for_one_data, facts, contexts)\n            futures.append((i, future))\n        \n        for index, future in tqdm(futures, total=len(futures)):\n            results[index] = future.result()\n    return results\n\ndef process_facts_for_one_data(facts, contexts):\n    result = []\n    for fact in facts:\n        fact_similarities = process_fact(fact, contexts)\n        result.append(fact_similarities)\n    return result\n\nresults = process_data_multithreaded(loaded_facts_results)\nprint(results[0])\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings \nimport openai\nfrom openai import OpenAI\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom datasets import load_dataset\ndataset = load_dataset('csv', data_files=\"/kaggle/input/complete-base-data/baseline_data_complete.csv\")\nclient = OpenAI(\n    api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n    base_url=\"https://api.openai.com/v1\"\n)\nclient.base_url=\"https://ai-yyds.com/v1\"\ndef get_text_embedding(text):\n    embedding = client.embeddings.create(input=text, model=\"text-embedding-3-large\"\n\n).data[0].embedding\n    return embedding\n\ndef calculate_cosine_similarity(vector1, vector2):\n    return cosine_similarity([vector1], [vector2])[0][0]\nsimilarities = []\nemb1 = get_text_embedding(\"Tenth Doctor has companion Martha Jones\")\nemb2 = get_text_embedding(\"STATS: Doctor(s): Tenth Companion(s): Martha Jones Episode(s): Daleks in Manhattan \")\nsimilarity = calculate_cosine_similarity(emb1,emb2)\nprint(similarity)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport openai\nimport csv\nimport time\nfrom datasets import load_dataset\nfrom threading import Semaphore\n\nMAX_RATE = 4  #\nsemaphore = Semaphore(MAX_RATE)\n    \ndataset_base = load_dataset(\"csv\", data_files = \"/kaggle/input/complete-base-data/baseline_data_complete.csv\")\n\ndef process_data(i, data):\n    \"\"\"处理单个数据项的函数\"\"\"\n    try:\n        with semaphore:\n            start_time = time.time()\n\n            report = generate_improved_judge_response(data)\n            facts = extract_facts(report) \n            elapsed_time = time.time() - start_time\n            if elapsed_time < 1 / MAX_RATE:\n                time.sleep(1 / MAX_RATE - elapsed_time)\n\n        return i, facts\n\n    except Exception as e:\n        print(f\"Error in process_data for index {i}: {e}\")\n        return i, {\"Ground Truth Facts\": [], \"Answer Facts\": []}  \n\nfacts_results = [None] * len(dataset_base['train'])\n\nwith ThreadPoolExecutor(max_workers=MAX_RATE) as executor:\n    futures = [executor.submit(process_data, indexes[i], dataset_base['train'][indexes[i]]) for i, data in enumerate(indexes)]\n\n    for future in tqdm(futures, total=len(indexes)):\n        try:\n            i, facts = future.result()\n            facts_results[i] = facts\n        except Exception as e:\n            print(f\"Error processing future: {e}\")\n            facts_results[i] = {\"Ground Truth Facts\": [], \"Answer Facts\": []} \n\nprint(\"Processing complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\nfrom tqdm import tqdm\n\ndef combine_fact(fact):\n    return f\"{fact['subject']} {fact['predicate']} {fact['object']}.\"\n\ndef calculate_similarity(context_embeddings, answer_embeddings):\n    similarities = []\n    \n    # Calculate cosine similarity between answer embeddings and context embeddings\n    cosine_sim = cosine_similarity(answer_embeddings, context_embeddings)\n    \n    # For each answer fact, get the max similarity score\n    max_similarities = cosine_sim.max(axis=1)\n    similarities.extend(max_similarities)\n    \n    return similarities\n    \n    # Initialize the Sentence Transformer model\nmodel = SentenceTransformer('all-MiniLM-L6-v2') \nresults = []\n# Sample context facts and answer facts\nfor facts in tqdm(loaded_facts_results):\n    context_facts = facts['Context Facts']\n    \n    answer_facts = facts['Answer Facts']\n\n    # Combine context facts into sentences\n    context_sentences = [combine_fact(fact) for fact in context_facts]\n    \n    # Combine answer facts into sentences\n    answer_sentences = [combine_fact(fact) for fact in answer_facts]\n    \n    context_embeddings = model.encode(context_sentences, convert_to_tensor=True)\n    answer_embeddings = model.encode(answer_sentences, convert_to_tensor=True)\n\n    similarities = calculate_similarity(context_embeddings, answer_embeddings)\n    similarities = [max(min(sim, 1), 0) for sim in similarities]\n    \n    # Calculate the standard deviation of the similarities\n    std_dev = np.std(similarities)\n    results.append({\"deviation\": std_dev, \"similarities\": similarities})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport openai\nimport csv\nimport time\nfrom datasets import load_dataset\nfrom threading import Semaphore\n\ndataset_base = load_dataset(\"csv\", data_files = \"/kaggle/input/long-context-complete/long_context_data_complete.csv\")\n\nMAX_RATE = 4  # 每秒最多的请求次数\nsemaphore = Semaphore(MAX_RATE)\n\ndef process_data(i, data):\n    with semaphore:\n        start_time = time.time()\n        report = generate_improved_judge_response(data)\n        facts = extract_facts(report)\n        elapsed_time = time.time() - start_time\n        if elapsed_time < 1 / MAX_RATE:\n            time.sleep(1 / MAX_RATE - elapsed_time)\n    return i, question\n\nquestions = [None] * len(dataset_base['train'])\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_data, i, dataset_base['train'][i]) for i, data in enumerate(dataset_base['train'])]\n    \n    for future in tqdm(futures, total=len(dataset_base['train'])):\n        try:\n            i, question = future.result()\n            questions[i] = question\n        except Exception as e:\n            print(f\"Error processing future: {e}\")\n            print(i)\n            questions[i] = \"None\"\n\noutput_file = '/kaggle/working/New_Questions_Long.csv'\n\n# Open a CSV file in write mode\nwith open(output_file, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    \n    writer.writerow(['Questions'])\n    \n    for i, (question) in enumerate(questions):\n        writer.writerow([question])\n\nprint(f\"Questions saved to {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nimport openai\nimport csv\nimport time\nfrom datasets import load_dataset\n\ndataset_base = load_dataset(\"csv\", data_files = \"/kaggle/input/complete-base-data/baseline_data_complete.csv\")\n\ndef process_data(i, data):\n    report = generate_improved_judge_response(data)\n    score = extract_judge_score(report, 'Factual Correctness:')\n    facts_answers = extract_judge_facts(report, 'Corresponding Atomic Factual For Answers:')\n    facts_grounds = extract_judge_facts(report, 'Corresponding Atomic Factual For Ground Truths:')\n    return i, score, facts_answers, facts_grounds\n\nscores = [None] * len(dataset_base['train'])\nfacts_answers = [None] * len(dataset_base['train'])\nfacts_grounds = [None] * len(dataset_base['train'])\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    futures = [executor.submit(process_data, i, dataset_base['train'][i]) for i, data in enumerate(dataset_base['train'])]\n    \n    for future in tqdm(futures, total=len(dataset_base['train'])):\n        try:\n            i, score, fact_answer, fact_ground = future.result()  # Get the result, which should be index and score\n            scores[i] = score\n            facts_answers[i] = fact_answer\n            facts_grounds[i] = fact_ground\n        except Exception as e:\n            print(f\"Error processing future: {e}\")\n            # Handle the error or assign a default value to that index (e.g., None)\n            print(i)\n            scores[i] = -1\n            facts_answers[i] = \"None\"\n            facts_grounds[i] = \"None\"\n        \noutput_file = '/kaggle/working/New_Factual_Correctness_Base.csv'\n\n# Open a CSV file in write mode\nwith open(output_file, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    \n    # 写入表头\n    writer.writerow(['Score', 'Facts_answers', 'Facts_ground_Truths'])\n    \n    # 写入每一行的索引、分数和事实\n    for i, (score, fact_answers, fact_grounds) in enumerate(zip(scores, facts_answers, facts_grounds)):\n        writer.writerow([score, fact_answers, fact_grounds])\n\nprint(f\"Factual Corrrectness data saved to {output_file}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor\nimport time\nimport csv\nfrom tqdm import tqdm\n\ndataset_base = load_dataset(\"csv\", data_files = \"/kaggle/input/long-context-complete/long_context_data_complete.csv\")\n\ndef process_data(i, data):\n    report = generate_improved_judge_response(data)\n    score = extract_judge_score(report, 'Factual Correctness:')\n    facts_answers = extract_judge_facts(report, 'Corresponding Atomic Factual For Answers:')\n    facts_grounds = extract_judge_facts(report, 'Corresponding Atomic Factual For Ground Truths:')\n    return i, score, facts_answers, facts_grounds\n    \ndef process_with_retries(i, data, max_retries=5, delay=1):\n    retries = 0\n    while retries < max_retries:\n        try:\n            return process_data(i, data)\n        except Exception as e:\n            if \"429\" in str(e): \n                retries += 1\n                print(f\"Retry {retries}/{max_retries} for index {i} due to {e}\")\n                time.sleep(delay * retries)\n            else:\n                raise\n    print(f\"Failed after {max_retries} retries for index {i}\")\n    return i, -1, \"None\", \"None\"\n\ndef process_in_batches(dataset, batch_size=100, max_workers=3, output_file='/kaggle/working/New_Factual_Correctness_Long.csv'):\n    scores = [None] * len(dataset_base['train'])\n    facts_answers = [None] * len(dataset_base['train'])\n    facts_grounds = [None] * len(dataset_base['train'])\n\n    total_data = len(dataset)\n    \n    # 按批次处理\n    for start in range(0, total_data, batch_size):\n        end = min(start + batch_size, total_data)\n        batch_data = dataset[start:end]\n        \n        print(f\"Processing batch: {start} to {end-1}\")\n        \n        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n            futures = [executor.submit(process_with_retries, i, data) for i, data in enumerate(batch_data)]\n            \n            for future in tqdm(futures, total=len(batch_data)):\n                index = None\n                try:\n                    index, score, fact_answer, fact_ground = future.result()\n                    scores[index] = score\n                    facts_answers[index] = fact_answer\n                    facts_grounds[index] = fact_ground\n                except Exception as e:\n                    print(f\"Error processing future: {e}\")\n                    if index != None:\n                        scores[index] = -1\n                        facts_answers[index] = \"None\"\n                        facts_grounds[index] = \"None\"\n\n    with open(output_file, mode='w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow(['Score', 'Facts_answers', 'Facts_ground_Truths'])\n        for i in sorted(scores.keys()):\n            writer.writerow([scores[i], facts_answers[i], facts_grounds[i]])\n    print(f\"Processing completed. Results saved to {output_file}\")\n\nprocess_in_batches(dataset_base['train'], batch_size=100, max_workers=3)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nfrom openai import OpenAI\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=\"sk-NMcE1XM1rCIO3XVa758b5045408f4c1eBa076e7aFc624f6d\",\n                openai_api_base=\"https://ai-yyds.com/v1\")\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\n\nquestions = []\nanswers = []\ncontexts = []\nreferences = []\nfor index in indexes:\n    question = test_ds[index]['question']\n    answer = rag_chain.invoke({\"input\": question})['answer']\n    context = [docs.page_content for docs in retriever.get_relevant_documents(question)]\n    reference = test_ds[index][\"answer\"]\n    questions.append(question)\n    answers.append(answer)\n    contexts.append(context)\n    references.append(reference)\n\n# To dict\ndata = {\n    \"question\":questions,\n    \"answer\": answers,\n    \"contexts\": contexts,\n    \"reference\": references\n}\nprint(references)\n# Convert dict to dataset\ndataset = Dataset.from_dict(data)   \n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ragas","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom ragas import evaluate\nfrom openai import OpenAI\nfrom langchain_openai import ChatOpenAI\nfrom datasets import Dataset, load_dataset\nfrom ragas.metrics import (\n    faithfulness,\n    ResponseRelevancy,\n    FactualCorrectness\n)\nfrom ragas.run_config import RunConfig\nfrom ragas.metrics.base import MetricWithLLM, MetricWithEmbeddings\nfrom ragas.llms import LangchainLLMWrapper\nfrom langchain_openai.chat_models import ChatOpenAI\nfrom langchain_openai.embeddings import OpenAIEmbeddings\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom ragas import RunConfig  \nfrom ragas.metrics import *  \nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings \nfrom ragas.llms import LangchainLLMWrapper  \nfrom ragas.embeddings import LangchainEmbeddingsWrapper  \nfrom langchain_openai import OpenAIEmbeddings\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom tqdm import tqdm\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nfrom ragas import SingleTurnSample \nimport asyncio\nfrom tqdm import tqdm\n#dataset = load_dataset('csv', data_file = \"/kaggle/input/complete-base-data/baseline_data_complete.csv\")\nemb = OpenAIEmbeddings(\n    model=\"text-embedding-3-large\",\n    openai_api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n    openai_api_base=\"https://ai-yyds.com/v1\"\n)\n\nllm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=\"sk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\",\n                openai_api_base=\"https://ai-yyds.com/v1\")\nllm_wrapper = LangchainLLMWrapper(llm)\nemb_wrapper= LangchainEmbeddingsWrapper(emb)\n\nresult = evaluate(dataset=dataset['train'],metrics=[Faithfulness()],llm=llm_wrapper, embeddings=emb_wrapper)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ragas import SingleTurnSample \nfrom ragas.metrics import ResponseRelevancy\n\nsample = SingleTurnSample(\n        user_input=\"When was the first super bowl?\",\n        response=\"The first superbowl was held on Jan 15, 1967\",\n        retrieved_contexts=[\n            \"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"\n        ]\n    )\n\nscorer = ResponseRelevancy(llm=evaluator_llm, embeddings=evaluator_embeddings)\nawait scorer.single_turn_ascore(sample)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ragas.dataset_schema import SingleTurnSample\nfrom ragas.metrics import SemanticSimilarity\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\n\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nimport os\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-NMcE1XM1rCIO3XVa758b5045408f4c1eBa076e7aFc624f6d\"\nos.environ[\"OPENAI_API_BASE\"] = \"https://ai-yyds.com/v1\"\nsk-zjhl7UX5N1fEXC1P83C84b5a154146EaA75e4f4aE933E9Ef\n# 初始化 OpenAI Embeddings（可以替换为您实际的 embedding 模型）\nevaluator_embedding = OpenAIEmbeddings()\n\n# 包装 embeddings\nembeddings_wrapper = LangchainEmbeddingsWrapper(evaluator_embedding)\n\nsample = SingleTurnSample(\n    response=\"The Eiffel Tower is located in Paris.\",\n    reference=\"The Eiffel Tower is located in Paris. It has a height of 1000ft.\"\n)\n\nscorer = SemanticSimilarity(embeddings=LangchainEmbeddingsWrapper(embeddings_wrapper))\nimport asyncio\nscore = asyncio.run(scorer.single_turn_ascore(sample))\n\n# 打印结果\nprint(\"Semantic Similarity Score:\", score)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}