# LLM Result Post-processing and Validation - RAG system evaluation
Large language modeling (LLM) has become a significant research focus and is utilized in various fields, such as text generation and dialog systems. One of the most essential applications of LLM is Retrieval Augmented Generation (RAG), which greatly enhances the reliability and relevance of generated content. However, evaluating RAG systems remains a challenging task. Traditional evaluation metrics struggle to effectively capture the key features of modern LLM-generated content, which often exhibits high fluency and naturalness. 

In this study, we propose an LLM-driven evaluation method for RAG systems. Our approach leverages LLMs to assess and analyze RAG performance across multiple dimensions, including Response Relevancy in the retrieval component , Factual Correctness andFaithfulness in the generation component. By incorporating these comprehensive evaluation criteria, we hope to gain a deeper understanding of the RAG system. In order to validate our proposed approach, we compare our results with a semantic similarity-based evaluation, which shows a strong correlation between the two. Furthermore, we compare our approach with RAGAS, a well-known RAG evaluation framework. We find similar patterns in specific evaluation metrics. Finally, we discuss potential future research directions for the evaluation of RAG systems Challenges and opportunities.

# Result







# Source Code

If you are intersted in the source code, please contact oskardong@outlook.com.
